#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
import re
from nltk.corpus import wordnet as wn
 
# DRY
def word_matches(h, ref):
	total = 0
#	print h
#	print ref
#	quit()
	for w in h:
#		hyp = wn.synsets(w)[0]
		if w in ref:
			total+=1
#		else:
#			for r in ref:
				#see how similar they are in wordnet
#				print(ref)
#				sim = wn.path_similarity(hyp, wn.synsets(r)[0])
#				if (sim > 0.1):
#					total+=sim
				#TODO: check to see if the same POS
	return total
#    return sum(1 for w in h (if w in ref or w in )
    # or sum(w in ref for w in f) # cast bool -> int
    # or sum(map(ref.__contains__, h)) # ugly!
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    # PEP8: use ' and not " for strings
    parser.add_argument('-i', '--input', default='data/train-test.hyp1-hyp2-ref',
            help='input file (default data/train-test.hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
		#fixed preprocessing to remove punctuation and make lowercase
                yield [re.findall(r"[\w']+",sentence.lower().strip()) for sentence in pair.split(' ||| ')]
 
    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        rset = set(ref)
        h1_match = word_matches(h1, rset)
        h2_match = word_matches(h2, rset)
        print(-1 if h1_match > h2_match else # \begin{cases}
                (0 if h1_match == h2_match
                    else 1)) # \end{cases}
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
